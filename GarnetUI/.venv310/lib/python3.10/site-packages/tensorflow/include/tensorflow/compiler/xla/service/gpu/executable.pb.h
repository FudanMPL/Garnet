// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: tensorflow/compiler/xla/service/gpu/executable.proto

#ifndef GOOGLE_PROTOBUF_INCLUDED_tensorflow_2fcompiler_2fxla_2fservice_2fgpu_2fexecutable_2eproto
#define GOOGLE_PROTOBUF_INCLUDED_tensorflow_2fcompiler_2fxla_2fservice_2fgpu_2fexecutable_2eproto

#include <limits>
#include <string>

#include <google/protobuf/port_def.inc>
#if PROTOBUF_VERSION < 3009000
#error This file was generated by a newer version of protoc which is
#error incompatible with your Protocol Buffer headers. Please update
#error your headers.
#endif
#if 3009002 < PROTOBUF_MIN_PROTOC_VERSION
#error This file was generated by an older version of protoc which is
#error incompatible with your Protocol Buffer headers. Please
#error regenerate this file with a newer version of protoc.
#endif

#include <google/protobuf/port_undef.inc>
#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/arena.h>
#include <google/protobuf/arenastring.h>
#include <google/protobuf/generated_message_table_driven.h>
#include <google/protobuf/generated_message_util.h>
#include <google/protobuf/inlined_string_field.h>
#include <google/protobuf/metadata.h>
#include <google/protobuf/generated_message_reflection.h>
#include <google/protobuf/message.h>
#include <google/protobuf/repeated_field.h>  // IWYU pragma: export
#include <google/protobuf/extension_set.h>  // IWYU pragma: export
#include <google/protobuf/unknown_field_set.h>
#include "tensorflow/compiler/xla/service/hlo.pb.h"
// @@protoc_insertion_point(includes)
#include <google/protobuf/port_def.inc>
#define PROTOBUF_INTERNAL_EXPORT_tensorflow_2fcompiler_2fxla_2fservice_2fgpu_2fexecutable_2eproto
PROTOBUF_NAMESPACE_OPEN
namespace internal {
class AnyMetadata;
}  // namespace internal
PROTOBUF_NAMESPACE_CLOSE

// Internal implementation detail -- do not use these members.
struct TableStruct_tensorflow_2fcompiler_2fxla_2fservice_2fgpu_2fexecutable_2eproto {
  static const ::PROTOBUF_NAMESPACE_ID::internal::ParseTableField entries[]
    PROTOBUF_SECTION_VARIABLE(protodesc_cold);
  static const ::PROTOBUF_NAMESPACE_ID::internal::AuxillaryParseTableField aux[]
    PROTOBUF_SECTION_VARIABLE(protodesc_cold);
  static const ::PROTOBUF_NAMESPACE_ID::internal::ParseTable schema[1]
    PROTOBUF_SECTION_VARIABLE(protodesc_cold);
  static const ::PROTOBUF_NAMESPACE_ID::internal::FieldMetadata field_metadata[];
  static const ::PROTOBUF_NAMESPACE_ID::internal::SerializationTable serialization_table[];
  static const ::PROTOBUF_NAMESPACE_ID::uint32 offsets[];
};
extern const ::PROTOBUF_NAMESPACE_ID::internal::DescriptorTable descriptor_table_tensorflow_2fcompiler_2fxla_2fservice_2fgpu_2fexecutable_2eproto;
namespace xla {
namespace gpu {
class XlaRuntimeGpuExecutableProto;
class XlaRuntimeGpuExecutableProtoDefaultTypeInternal;
extern XlaRuntimeGpuExecutableProtoDefaultTypeInternal _XlaRuntimeGpuExecutableProto_default_instance_;
}  // namespace gpu
}  // namespace xla
PROTOBUF_NAMESPACE_OPEN
template<> ::xla::gpu::XlaRuntimeGpuExecutableProto* Arena::CreateMaybeMessage<::xla::gpu::XlaRuntimeGpuExecutableProto>(Arena*);
PROTOBUF_NAMESPACE_CLOSE
namespace xla {
namespace gpu {

// ===================================================================

class XlaRuntimeGpuExecutableProto :
    public ::PROTOBUF_NAMESPACE_ID::Message /* @@protoc_insertion_point(class_definition:xla.gpu.XlaRuntimeGpuExecutableProto) */ {
 public:
  XlaRuntimeGpuExecutableProto();
  virtual ~XlaRuntimeGpuExecutableProto();

  XlaRuntimeGpuExecutableProto(const XlaRuntimeGpuExecutableProto& from);
  XlaRuntimeGpuExecutableProto(XlaRuntimeGpuExecutableProto&& from) noexcept
    : XlaRuntimeGpuExecutableProto() {
    *this = ::std::move(from);
  }

  inline XlaRuntimeGpuExecutableProto& operator=(const XlaRuntimeGpuExecutableProto& from) {
    CopyFrom(from);
    return *this;
  }
  inline XlaRuntimeGpuExecutableProto& operator=(XlaRuntimeGpuExecutableProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::PROTOBUF_NAMESPACE_ID::Descriptor* GetDescriptor() {
    return GetMetadataStatic().descriptor;
  }
  static const ::PROTOBUF_NAMESPACE_ID::Reflection* GetReflection() {
    return GetMetadataStatic().reflection;
  }
  static const XlaRuntimeGpuExecutableProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const XlaRuntimeGpuExecutableProto* internal_default_instance() {
    return reinterpret_cast<const XlaRuntimeGpuExecutableProto*>(
               &_XlaRuntimeGpuExecutableProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    0;

  friend void swap(XlaRuntimeGpuExecutableProto& a, XlaRuntimeGpuExecutableProto& b) {
    a.Swap(&b);
  }
  inline void Swap(XlaRuntimeGpuExecutableProto* other) {
    if (other == this) return;
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  inline XlaRuntimeGpuExecutableProto* New() const final {
    return CreateMaybeMessage<XlaRuntimeGpuExecutableProto>(nullptr);
  }

  XlaRuntimeGpuExecutableProto* New(::PROTOBUF_NAMESPACE_ID::Arena* arena) const final {
    return CreateMaybeMessage<XlaRuntimeGpuExecutableProto>(arena);
  }
  void CopyFrom(const ::PROTOBUF_NAMESPACE_ID::Message& from) final;
  void MergeFrom(const ::PROTOBUF_NAMESPACE_ID::Message& from) final;
  void CopyFrom(const XlaRuntimeGpuExecutableProto& from);
  void MergeFrom(const XlaRuntimeGpuExecutableProto& from);
  PROTOBUF_ATTRIBUTE_REINITIALIZES void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  #if GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER
  const char* _InternalParse(const char* ptr, ::PROTOBUF_NAMESPACE_ID::internal::ParseContext* ctx) final;
  #else
  bool MergePartialFromCodedStream(
      ::PROTOBUF_NAMESPACE_ID::io::CodedInputStream* input) final;
  #endif  // GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER
  void SerializeWithCachedSizes(
      ::PROTOBUF_NAMESPACE_ID::io::CodedOutputStream* output) const final;
  ::PROTOBUF_NAMESPACE_ID::uint8* InternalSerializeWithCachedSizesToArray(
      ::PROTOBUF_NAMESPACE_ID::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  inline void SharedCtor();
  inline void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(XlaRuntimeGpuExecutableProto* other);
  friend class ::PROTOBUF_NAMESPACE_ID::internal::AnyMetadata;
  static ::PROTOBUF_NAMESPACE_ID::StringPiece FullMessageName() {
    return "xla.gpu.XlaRuntimeGpuExecutableProto";
  }
  private:
  inline ::PROTOBUF_NAMESPACE_ID::Arena* GetArenaNoVirtual() const {
    return nullptr;
  }
  inline void* MaybeArenaPtr() const {
    return nullptr;
  }
  public:

  ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadata() const final;
  private:
  static ::PROTOBUF_NAMESPACE_ID::Metadata GetMetadataStatic() {
    ::PROTOBUF_NAMESPACE_ID::internal::AssignDescriptors(&::descriptor_table_tensorflow_2fcompiler_2fxla_2fservice_2fgpu_2fexecutable_2eproto);
    return ::descriptor_table_tensorflow_2fcompiler_2fxla_2fservice_2fgpu_2fexecutable_2eproto.file_level_metadata[kIndexInFileMessages];
  }

  public:

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  enum : int {
    kGpuAsmTextFieldNumber = 2,
    kGpuBinaryFieldNumber = 3,
    kXlaRuntimeExecutableFieldNumber = 1,
  };
  // string gpu_asm_text = 2;
  void clear_gpu_asm_text();
  const std::string& gpu_asm_text() const;
  void set_gpu_asm_text(const std::string& value);
  void set_gpu_asm_text(std::string&& value);
  void set_gpu_asm_text(const char* value);
  void set_gpu_asm_text(const char* value, size_t size);
  std::string* mutable_gpu_asm_text();
  std::string* release_gpu_asm_text();
  void set_allocated_gpu_asm_text(std::string* gpu_asm_text);

  // bytes gpu_binary = 3;
  void clear_gpu_binary();
  const std::string& gpu_binary() const;
  void set_gpu_binary(const std::string& value);
  void set_gpu_binary(std::string&& value);
  void set_gpu_binary(const char* value);
  void set_gpu_binary(const void* value, size_t size);
  std::string* mutable_gpu_binary();
  std::string* release_gpu_binary();
  void set_allocated_gpu_binary(std::string* gpu_binary);

  // .xla.XlaRuntimeExecutableProto xla_runtime_executable = 1;
  bool has_xla_runtime_executable() const;
  void clear_xla_runtime_executable();
  const ::xla::XlaRuntimeExecutableProto& xla_runtime_executable() const;
  ::xla::XlaRuntimeExecutableProto* release_xla_runtime_executable();
  ::xla::XlaRuntimeExecutableProto* mutable_xla_runtime_executable();
  void set_allocated_xla_runtime_executable(::xla::XlaRuntimeExecutableProto* xla_runtime_executable);

  // @@protoc_insertion_point(class_scope:xla.gpu.XlaRuntimeGpuExecutableProto)
 private:
  class _Internal;

  ::PROTOBUF_NAMESPACE_ID::internal::InternalMetadataWithArena _internal_metadata_;
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr gpu_asm_text_;
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr gpu_binary_;
  ::xla::XlaRuntimeExecutableProto* xla_runtime_executable_;
  mutable ::PROTOBUF_NAMESPACE_ID::internal::CachedSize _cached_size_;
  friend struct ::TableStruct_tensorflow_2fcompiler_2fxla_2fservice_2fgpu_2fexecutable_2eproto;
};
// ===================================================================


// ===================================================================

#ifdef __GNUC__
  #pragma GCC diagnostic push
  #pragma GCC diagnostic ignored "-Wstrict-aliasing"
#endif  // __GNUC__
// XlaRuntimeGpuExecutableProto

// .xla.XlaRuntimeExecutableProto xla_runtime_executable = 1;
inline bool XlaRuntimeGpuExecutableProto::has_xla_runtime_executable() const {
  return this != internal_default_instance() && xla_runtime_executable_ != nullptr;
}
inline const ::xla::XlaRuntimeExecutableProto& XlaRuntimeGpuExecutableProto::xla_runtime_executable() const {
  const ::xla::XlaRuntimeExecutableProto* p = xla_runtime_executable_;
  // @@protoc_insertion_point(field_get:xla.gpu.XlaRuntimeGpuExecutableProto.xla_runtime_executable)
  return p != nullptr ? *p : *reinterpret_cast<const ::xla::XlaRuntimeExecutableProto*>(
      &::xla::_XlaRuntimeExecutableProto_default_instance_);
}
inline ::xla::XlaRuntimeExecutableProto* XlaRuntimeGpuExecutableProto::release_xla_runtime_executable() {
  // @@protoc_insertion_point(field_release:xla.gpu.XlaRuntimeGpuExecutableProto.xla_runtime_executable)
  
  ::xla::XlaRuntimeExecutableProto* temp = xla_runtime_executable_;
  xla_runtime_executable_ = nullptr;
  return temp;
}
inline ::xla::XlaRuntimeExecutableProto* XlaRuntimeGpuExecutableProto::mutable_xla_runtime_executable() {
  
  if (xla_runtime_executable_ == nullptr) {
    auto* p = CreateMaybeMessage<::xla::XlaRuntimeExecutableProto>(GetArenaNoVirtual());
    xla_runtime_executable_ = p;
  }
  // @@protoc_insertion_point(field_mutable:xla.gpu.XlaRuntimeGpuExecutableProto.xla_runtime_executable)
  return xla_runtime_executable_;
}
inline void XlaRuntimeGpuExecutableProto::set_allocated_xla_runtime_executable(::xla::XlaRuntimeExecutableProto* xla_runtime_executable) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == nullptr) {
    delete reinterpret_cast< ::PROTOBUF_NAMESPACE_ID::MessageLite*>(xla_runtime_executable_);
  }
  if (xla_runtime_executable) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
      reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(xla_runtime_executable)->GetArena();
    if (message_arena != submessage_arena) {
      xla_runtime_executable = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, xla_runtime_executable, submessage_arena);
    }
    
  } else {
    
  }
  xla_runtime_executable_ = xla_runtime_executable;
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.XlaRuntimeGpuExecutableProto.xla_runtime_executable)
}

// string gpu_asm_text = 2;
inline void XlaRuntimeGpuExecutableProto::clear_gpu_asm_text() {
  gpu_asm_text_.ClearToEmptyNoArena(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
}
inline const std::string& XlaRuntimeGpuExecutableProto::gpu_asm_text() const {
  // @@protoc_insertion_point(field_get:xla.gpu.XlaRuntimeGpuExecutableProto.gpu_asm_text)
  return gpu_asm_text_.GetNoArena();
}
inline void XlaRuntimeGpuExecutableProto::set_gpu_asm_text(const std::string& value) {
  
  gpu_asm_text_.SetNoArena(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:xla.gpu.XlaRuntimeGpuExecutableProto.gpu_asm_text)
}
inline void XlaRuntimeGpuExecutableProto::set_gpu_asm_text(std::string&& value) {
  
  gpu_asm_text_.SetNoArena(
    &::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:xla.gpu.XlaRuntimeGpuExecutableProto.gpu_asm_text)
}
inline void XlaRuntimeGpuExecutableProto::set_gpu_asm_text(const char* value) {
  GOOGLE_DCHECK(value != nullptr);
  
  gpu_asm_text_.SetNoArena(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:xla.gpu.XlaRuntimeGpuExecutableProto.gpu_asm_text)
}
inline void XlaRuntimeGpuExecutableProto::set_gpu_asm_text(const char* value, size_t size) {
  
  gpu_asm_text_.SetNoArena(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:xla.gpu.XlaRuntimeGpuExecutableProto.gpu_asm_text)
}
inline std::string* XlaRuntimeGpuExecutableProto::mutable_gpu_asm_text() {
  
  // @@protoc_insertion_point(field_mutable:xla.gpu.XlaRuntimeGpuExecutableProto.gpu_asm_text)
  return gpu_asm_text_.MutableNoArena(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
}
inline std::string* XlaRuntimeGpuExecutableProto::release_gpu_asm_text() {
  // @@protoc_insertion_point(field_release:xla.gpu.XlaRuntimeGpuExecutableProto.gpu_asm_text)
  
  return gpu_asm_text_.ReleaseNoArena(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
}
inline void XlaRuntimeGpuExecutableProto::set_allocated_gpu_asm_text(std::string* gpu_asm_text) {
  if (gpu_asm_text != nullptr) {
    
  } else {
    
  }
  gpu_asm_text_.SetAllocatedNoArena(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), gpu_asm_text);
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.XlaRuntimeGpuExecutableProto.gpu_asm_text)
}

// bytes gpu_binary = 3;
inline void XlaRuntimeGpuExecutableProto::clear_gpu_binary() {
  gpu_binary_.ClearToEmptyNoArena(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
}
inline const std::string& XlaRuntimeGpuExecutableProto::gpu_binary() const {
  // @@protoc_insertion_point(field_get:xla.gpu.XlaRuntimeGpuExecutableProto.gpu_binary)
  return gpu_binary_.GetNoArena();
}
inline void XlaRuntimeGpuExecutableProto::set_gpu_binary(const std::string& value) {
  
  gpu_binary_.SetNoArena(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:xla.gpu.XlaRuntimeGpuExecutableProto.gpu_binary)
}
inline void XlaRuntimeGpuExecutableProto::set_gpu_binary(std::string&& value) {
  
  gpu_binary_.SetNoArena(
    &::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:xla.gpu.XlaRuntimeGpuExecutableProto.gpu_binary)
}
inline void XlaRuntimeGpuExecutableProto::set_gpu_binary(const char* value) {
  GOOGLE_DCHECK(value != nullptr);
  
  gpu_binary_.SetNoArena(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:xla.gpu.XlaRuntimeGpuExecutableProto.gpu_binary)
}
inline void XlaRuntimeGpuExecutableProto::set_gpu_binary(const void* value, size_t size) {
  
  gpu_binary_.SetNoArena(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:xla.gpu.XlaRuntimeGpuExecutableProto.gpu_binary)
}
inline std::string* XlaRuntimeGpuExecutableProto::mutable_gpu_binary() {
  
  // @@protoc_insertion_point(field_mutable:xla.gpu.XlaRuntimeGpuExecutableProto.gpu_binary)
  return gpu_binary_.MutableNoArena(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
}
inline std::string* XlaRuntimeGpuExecutableProto::release_gpu_binary() {
  // @@protoc_insertion_point(field_release:xla.gpu.XlaRuntimeGpuExecutableProto.gpu_binary)
  
  return gpu_binary_.ReleaseNoArena(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited());
}
inline void XlaRuntimeGpuExecutableProto::set_allocated_gpu_binary(std::string* gpu_binary) {
  if (gpu_binary != nullptr) {
    
  } else {
    
  }
  gpu_binary_.SetAllocatedNoArena(&::PROTOBUF_NAMESPACE_ID::internal::GetEmptyStringAlreadyInited(), gpu_binary);
  // @@protoc_insertion_point(field_set_allocated:xla.gpu.XlaRuntimeGpuExecutableProto.gpu_binary)
}

#ifdef __GNUC__
  #pragma GCC diagnostic pop
#endif  // __GNUC__

// @@protoc_insertion_point(namespace_scope)

}  // namespace gpu
}  // namespace xla

// @@protoc_insertion_point(global_scope)

#include <google/protobuf/port_undef.inc>
#endif  // GOOGLE_PROTOBUF_INCLUDED_GOOGLE_PROTOBUF_INCLUDED_tensorflow_2fcompiler_2fxla_2fservice_2fgpu_2fexecutable_2eproto
