import nn as nn
from tensor import Tensor,autograd_function
import optimizer as optim
import dataloader as dataloader
import functional as F
program.use_trunc_pr = True

class LeNet(nn.Module):
    def __init__(self):
        super(LeNet, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 6, 5), # in_channels, out_channels, kernel_size
            nn.Relu(),
            nn.AvgPool2d(2, 2), # kernel_size, stride
            nn.Conv2d(6, 16, 5),
            nn.Relu(),
            nn.AvgPool2d(2, 2))
        self.fc = nn.Sequential(
            nn.Linear(16*4*4, 120),
            nn.Relu(),
            nn.Linear(120, 840),
            nn.Relu())
        self.fc1 = nn.Linear(840, 10)
    def forward(self, img):
        feature = self.conv(img)
        feature = feature.view(img.shape[0], -1)
        output = self.fc(feature)
        output = self.fc1(output)
        return output

loss = nn.CrossEntropyLoss()
x = Tensor.ones(128, 1, 28, 28)
labels = Tensor.ones(128, 10)
dataload = dataloader.DataLoader(x, labels, batch_size = 8)
model = LeNet()
model.train()
optimizer = optim.SGD(model.parameters(), lr = 0.01)
crossloss = nn.CrossEntropyLoss()
@for_range(1)
def _(i):
    input, label = dataload.get_data(0)
    output = model(input)
    loss = crossloss(output, label)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

