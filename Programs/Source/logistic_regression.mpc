m = 32 # 特征数
train_number_from_0 = 2 # 第0方提供的训练样本数量
train_number_from_1 = 0 # 第1方提供的训练样本数量
n_test = 2 # 测试样本数量,假设测试样本全部来自第0方
learning_rate = 0.001 # 学习率
epoch = 100 # 迭代的轮次


train_y = sint.Array(train_number_from_0 + train_number_from_1)
train_x = sfix.Matrix(train_number_from_0 + train_number_from_1, m)
test_y = sint.Array(n_test)
test_x = sfix.Matrix(n_test, m)
weight = sfix.Array(m)



for i in range(train_number_from_0):
    for j in range(m):
        train_x[i][j].get_input_from(0)
        print_ln("%s", train_x[i][j].reveal())
    train_y[i].get_input_from(0)

for i in range(train_number_from_0, train_number_from_0 + train_number_from_1):
    for j in range(m):
        train_x[i][j].get_input_from(1)
    train_y[i].get_input_from(1)

for i in range(n_test):
    for j in range(m):
        test_x[i][j].get_input_from(2)
    test_y[i].get_input_from(2)






debug_point()

def sigmoid(val):
    val = val.get_vector() + 0.5
    sign1 = val < 0
    sign2 = val > 1
    val = val * ( 1 - sign1) * ( 1- sign2) + sign2 # 只有当 val + 0.5 处于[0,1]，才是原值。小于0是，则为0，大于1时则恒为1
    return val

for i in range(epoch):
    pred = train_x.dot(weight)
    pred = sigmoid(pred)
    loss = train_y - pred
    temp = pred * (1 - pred)
    temp = temp * loss
    gradient = train_x.transpose().dot(temp)
    weight = weight - gradient * learning_rate
    print_ln("loss = %s", sum(loss.reveal()) / len(train_y))

test_pred = sigmoid(test_x.dot(weight))
test_pred = test_pred.get_vector().v.round(64, sfix.f, nearest=True)

pred_plaintext = test_pred.reveal()
test_y_plaintext = test_y.reveal()
correct = 0
for i in range(n_test):
    correct = correct + (pred_plaintext[i] == test_y_plaintext[i])

print_ln('accuracy: %s/%s', sum(correct), n_test)


