# this trains LeNet on MNIST with a dropout layer
# see https://github.com/csiro-mlai/mnist-mpc for data preparation

program.options_from_args()

training_samples = MultiArray([60000, 28, 28], sfix)
training_labels = MultiArray([60000, 10], sint)

test_samples = MultiArray([10000, 28, 28], sfix)
test_labels = MultiArray([10000, 10], sint)

training_labels.input_from(0)
training_samples.input_from(0)

test_labels.input_from(0)
test_samples.input_from(0)

from Compiler import ml
tf = ml

layers = [
    tf.keras.layers.BatchNorm2d(),
    tf.keras.layers.Conv2D(20, 5, 2, padding=2, activation='relu'),
]


'''
if 'batchnorm' in program.args:
    layers += [tf.keras.layers.BatchNormalization()]

layers += [
    tf.keras.layers.MaxPooling2D(2),
    tf.keras.layers.Conv2D(50, 5, 1, 'valid', activation='relu'),
]
'''


if 'batchnorm' in program.args:
    layers += [tf.keras.layers.BatchNormalization()]


layers += [
    tf.keras.layers.Dense(10, activation='softmax')
]

model = tf.keras.models.Sequential(layers)

optim = tf.keras.optimizers.Adam(amsgrad=True)

model.compile(optimizer=optim)
start_profiling()
opt = model.fit(
    training_samples,
    training_labels,
    epochs=1,
    batch_size=128,
    validation_data=(test_samples, test_labels)
)
stop_profiling()
for var in model.trainable_variables:
    var.write_to_file()
