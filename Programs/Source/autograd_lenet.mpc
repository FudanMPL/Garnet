
import Compiler.nn as nn
from Compiler.tensor import Tensor,autograd_function
import Compiler.optimizer as optim
import Compiler.dataloader as dataloader
import Compiler.functional as F
program.use_trunc_pr = True

class LeNet(nn.Module):
    def __init__(self):
        super(LeNet, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 6, 5), # in_channels, out_channels, kernel_size
            nn.Relu(),
            nn.AvgPool2d(2, 2), # kernel_size, stride
            nn.Conv2d(6, 16, 5),
            nn.Relu(),
            nn.AvgPool2d(2, 2))
        self.fc = nn.Sequential(
            nn.Linear(16*20*4, 120),
            nn.Relu(),
            nn.Linear(120, 84),
            nn.Relu())
        self.fc1 = nn.Linear(84, 10)
    def forward(self, img):
        feature = self.conv(img)
        feature = feature.view(img.shape[0], -1)
        output = self.fc(feature)
        output = self.fc1(output)
        return output


model = LeNet()
crossloss = nn.CrossEntropyLoss()

x = MultiArray([1,1,28,28], sfix)
y = MultiArray([1,10], sfix)

# @for_range(x.total_size())
# def _(i):
#     x.assign_vector(sfix(i/10), i)
# y.assign_all(0)
# for i in range(10):
#     y[i][i//3] = 1

dataload = dataloader.DataLoader(x, y, batch_size = 1)
optimizer = optim.SGD(model.parameters(), lr = 0.01)
params = list(model.parameters())
for i in params:
    i.assign_all(0.1)
model.train()

@for_range(1)
def _(i):
    input, label = dataload.get_data(0)
    output = model(input)
    loss = crossloss(output, label)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
   