from tensor import Tensor,autograd_function
import tensor
import functional as F
from Compiler.types import *
import copy
program.options_from_args()

# sfix.set_precision(23, 40)
# cfix.set_precision(12, 40)

@autograd_function
def test_mul():
    print_ln('test_mul_input: ')
    x = MultiArray([3, 2, 3], sfix)
    for i in range(x.total_size()):
        x.assign_vector(sfix(i), i)
    x.print_reveal_nested()
    y = MultiArray([2, 1], cint)
    y.assign_all(5)
    y[0][0]=6
    y.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)
    input2 = Tensor(y, req_grad = False)

    input3 = input1 * input2

    tensor.train()
    tensor.reset_op_id()

    print_ln('test_mul_output: ')
    input3 = input1 * input2
    input3.value.print_reveal_nested()
    input3.backward()

    print_ln('test_mul_backward: ')
    input1.grad.print_reveal_nested()
    #input2.grad.print_reveal_nested()
    
def test_single_operation(func,Value):
    Value.print_reveal_nested()
    input = Tensor(Value, req_grad = True)
    output = getattr(input, func)() 
    tensor.train()
    tensor.reset_op_id()

    input3 = input1 *input2
    input3.value.print_reveal_nested()
    input3.backward()
    input1.grad.print_reveal_nested()
    input2.grad.print_reveal_nested()

@autograd_function
def test_add():
    print_ln('test_add_input: ')
    x = MultiArray([2, 7, 1], sfix)
    for i in range(x.total_size()):
        x.assign_vector(sfix(i), i)
    x.print_reveal_nested()
    y = MultiArray([2, 7, 20], sfix)
    y.assign_all(5)
    y[0][0]=6
    y.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)
    input2 = Tensor(y, req_grad = True)

    # input3 = input1 +input2

    # tensor.train()
    # tensor.reset_op_id()

    print_ln('test_add_output: ')
    input3 = input1 +input2
    input3.value.print_reveal_nested()
    input3.backward()
    print_ln('test_add_backward: ')
    input1.grad.print_reveal_nested()
    input2.grad.print_reveal_nested()

@autograd_function
def test_sub():
    print_ln('test_sub_input: ')
    x = MultiArray([2, 7, 1], sfix)
    x.assign_all(1)
    x.print_reveal_nested()
    y = MultiArray([2, 7, 20], sfix)
    y.assign_all(5)
    y[0][0]=6
    y.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)
    input2 = Tensor(y, req_grad = True)

    # input3 = input1 - input2

    # tensor.train()
    # tensor.reset_op_id()

    print_ln('test_sub_output: ')
    input3 = input1 - input2
    input3.value.print_reveal_nested()
    input3.backward()
    print_ln('test_sub_backward: ')
    input1.grad.print_reveal_nested()
    input2.grad.print_reveal_nested()

@autograd_function
def test_div():
    print_ln('test_div_input: ')
    x = MultiArray([2, 2, 2, 2], sfix)
    y = MultiArray([1, 2, 1, 1], sfix)
    xx = [[[[-0.0150, -0.0050],
          [-0.0050,  0.0050]],

         [[-0.0162, -0.0062],
          [-0.0062,  0.0038]]],


        [[[-0.0050,  0.0050],
          [ 0.0050,  0.0150]],

         [[-0.0062,  0.0038],
          [ 0.0038,  0.0238]]]]
    yy = [[[[0.0093]],

         [[0.0119]]]]
    for i in range(0,2):
        for j in range(0,2):
            for k in range(0,2):
                for l in range(0,2):
                    x[i][j][k][l] = sfix(xx[i][j][k][l])
    for i in range(0,1):
        for j in range(0,2):
            for k in range(0,1):
                for l in range(0,1):
                    y[i][j][k][l] = sfix(yy[i][j][k][l])
    x.print_reveal_nested()
    y.print_reveal_nested()

    input1 = Tensor(x, req_grad = True)
    input2 = Tensor(y, req_grad = True)

    # input3 = input1 / input2

    # tensor.train()
    # tensor.reset_op_id()

    print_ln('test_div_output: ')
    input3 = input1 / input2
    input3.value.print_reveal_nested()
    input3.backward()
    print_ln('test_div_backward: ')
    input1.grad.print_reveal_nested()
    input2.grad.print_reveal_nested()

@autograd_function
def test_abs():
    print_ln('test_abs')
    x = MultiArray([3, 3], sfix)
    for i in range(0,3):
        for  j in range(0,3):
            x[i][j] = sfix(i-j)
    x.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)
    # input2 = input1.abs()
    # tensor.train()
    # tensor.reset_op_id()
    input2 = input1.abs()
    input2.value.print_reveal_nested()
    input2.backward()
    input1.grad.print_reveal_nested()

@autograd_function
def test_argmax(dim):
    print_ln('test_argmax')
    i, j = 5, 5
    x = MultiArray([i, j], sfix)
    for i in range(0,5):
        for  j in range(0,5):
            x[i][j] = sfix((i*17+j*11)%5)
    x.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)

    input2 = input1.argmax(dim=dim)

    input2.value.print_reveal_nested()

@autograd_function
def test_combine():
    print_ln('test_combine')
    i, j = 3, 3
    x = MultiArray([i, j], sfix)
    for i in range(0,3):
        for  j in range(0,3):
            x[i][j] = sfix(i-j)
    xMultiArray_2_1 = MultiArray([3, 3], sfix)
    for i in range(0,3):
        for  j in range(0,3):
            xMultiArray_2_1[i][j] = sfix(2)
    x.print_reveal_nested()
    xMultiArray_2_1.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)
    input0 = Tensor(xMultiArray_2_1, req_grad = True)

    input8 =  input1 * input0 
    input8.value.print_reveal_nested()
    input2 = input8.abs()
    input2.value.print_reveal_nested()
    input3 = input2.exp()
    input3.value.print_reveal_nested()
    input4 = input3.mean()
    input4.value.print_reveal_nested()    
    input4.backward()
    input1.grad.print_reveal_nested()
    input8.grad.print_reveal_nested()    
    input2.grad.print_reveal_nested()
    input3.grad.print_reveal_nested()

@autograd_function
def test_exp():
    print_ln('test_exp')
    i, j = 3, 3
    x = MultiArray([i, j], sfix)
    for i in range(0,3):
        for  j in range(0,3):
            x[i][j] = sfix(i+j)
    x.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)

    # input2 = input1.exp()

    # tensor.train()
    # tensor.reset_op_id()

    input2 = input1.exp()
    input2.backward()
    input2.value.print_reveal_nested()
    input1.grad.print_reveal_nested()

@autograd_function
def test_log():
    print_ln('test_log')
    i, j = 3, 3
    x = MultiArray([i, j], sfix)
    for i in range(0,3):
        for  j in range(0,3):
            x[i][j] = sfix(i+j)
    x.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)

    # input2 = input1.log()

    # tensor.train()
    # tensor.reset_op_id()

    input2 = input1.log()
    input2.backward()
    input2.value.print_reveal_nested()
    input1.grad.print_reveal_nested()

@autograd_function
def test_pow(pow):
    i, j = 3, 3
    x = MultiArray([i, j], sfix)
    for i in range(0,3):
        for  j in range(0,3):
            x[i][j] = sfix(i+j)
    x.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)

    # input2 = input1.pow(pow)

    # tensor.train()
    # tensor.reset_op_id()

    input2 = input1.pow(pow)
    input2.backward()
    input2.value.print_reveal_nested()
    input1.grad.print_reveal_nested()

@autograd_function
def test_invsqrt():
    i, j = 3, 3
    x = MultiArray([i, j], sfix)
    for i in range(0,3):
        for  j in range(0,3):
            x[i][j] = sfix(i+j)
    x.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)

    # input2 = input1.invsqrt()

    # tensor.train()
    # tensor.reset_op_id()

    input2 = input1.invsqrt()
    input2.backward()
    input2.value.print_reveal_nested()
    input1.grad.print_reveal_nested()

@autograd_function
def test_sum(dim, keepdim=False):
    print_ln('test_sum')
    x = MultiArray([3, 3, 3, 3], sfix)
    for i in range(0,3):
        for j in range(0,3):
            for k in range(0, 3):
                for q in range(0, 3):
                    x[i][j][k][q] = sfix((i*j*k*q+i+j+k+q)*1e-2)
    x.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)

    # input2 = input1.sum(dim, keepdim)

    # tensor.train()
    # tensor.reset_op_id()

    input2 = input1.sum(dim, keepdim)
    input2.backward()
    input2.value.print_reveal_nested()
    input1.grad.print_reveal_nested()

@autograd_function
def test_mean(dim, keepdim=False):
    print_ln('test_mean')
    x = MultiArray([3, 3, 3, 3], sfix)
    for i in range(0,3):
        for j in range(0,3):
            for k in range(0, 3):
                for q in range(0, 3):
                    x[i][j][k][q] = sfix((i*j*k*q+i+j+k+q)*1e-2)
    x.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)

    # input2 = input1.var(dim, keepdim)

    # tensor.train()
    # tensor.reset_op_id()

    input2 = input1.mean(dim, keepdim)
    input2.backward()
    input2.value.print_reveal_nested()
    input1.grad.print_reveal_nested()
    

@autograd_function
def test_var(dim, keepdim=False):
    print_ln('test_var')
    x = MultiArray([3,4, 5, 6], sfix)
    for i in range(0,3):
        for j in range(0,4):
            for k in range(0, 5):
                for q in range(0, 6):
                    x[i][j][k][q] = sfix((i*j*k*q+i+j+k+q))
    x.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)

    # input2 = input1.var(dim, keepdim)

    # tensor.train()
    # tensor.reset_op_id()

    input2 = input1.var(dim, keepdim)
    input2.backward()
    input2.value.print_reveal_nested()
    input1.grad.print_reveal_nested()

@autograd_function
def test_std(dim, keepdim=False):
    print_ln('test_std')
    x = MultiArray([2,2,2,2], sfix)
    for i in range(0,2):
        for j in range(0,2):
            for k in range(0, 2):
                for q in range(0, 2):
                    x[i][j][k][q] = sfix((i*j*k*q+i+j+k+q)%101*1e-2)
    x.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)

    # input2 = input1.std(dim, keepdim)

    # tensor.train()
    # tensor.reset_op_id()

    input2 = input1.std(dim, keepdim)
    input2.backward()
    input2.value.print_reveal_nested()
    input1.grad.print_reveal_nested()

@autograd_function
def test_addc():
    print_ln('test_addc')
    i, j = 3, 3
    x = MultiArray([i, j], sfix)
    for i in range(0,3):
        for  j in range(0,3):
            x[i][j] = sfix(i+j)
    x.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)

    # input2 = input1 + 1

    # tensor.train()
    # tensor.reset_op_id()

    input2 = input1 + 1
    input2.backward()
    input2.value.print_reveal_nested()
    input1.grad.print_reveal_nested()

@autograd_function
def test_subc():
    print_ln('test_subc')
    i, j = 3, 3
    x = MultiArray([i, j], sfix)
    for i in range(0,3):
        for  j in range(0,3):
            x[i][j] = sfix(i+j)
    x.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)


    input2 = input1 - 1
    input2.backward()
    input2.value.print_reveal_nested()
    input1.grad.print_reveal_nested()

@autograd_function
def test_mulc():
    print_ln('test_mulc')
    i, j = 3, 3
    x = MultiArray([i, j], sfix)
    for i in range(0,3):
        for  j in range(0,3):
            x[i][j] = sfix(i+j)
    x.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)

    # input2 = input1 * 2

    # tensor.train()
    # tensor.reset_op_id()

    input2 = input1 * 2
    input2.backward()
    input2.value.print_reveal_nested()
    input1.grad.print_reveal_nested()

@autograd_function
def test_divc():
    print_ln('test_divc')
    i, j = 3, 3
    x = MultiArray([i, j], sfix)
    for i in range(0,3):
        for  j in range(0,3):
            x[i][j] = sfix(i+j)
    x.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)

    # input2 = input1 / 2

    # tensor.train()
    # tensor.reset_op_id()

    input2 = input1 / 2
    input2.backward()
    input2.value.print_reveal_nested()
    input1.grad.print_reveal_nested()

@autograd_function
def test_neg():
    print_ln('test_neg')
    i, j = 3, 3
    x = MultiArray([i, j], sfix)
    for i in range(0,3):
        for  j in range(0,3):
            x[i][j] = sfix(i+j)
    x.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)

    # input2 = -input1

    # tensor.train()
    # tensor.reset_op_id()

    input2 = -input1
    input2.backward()
    input2.value.print_reveal_nested()
    input1.grad.print_reveal_nested()

@autograd_function
def test_f_droupout(p):
    print_ln('test_droupout')
    i, j = 3, 3
    x = MultiArray([i, j], sfix)
    for i in range(0,i):
        for  j in range(0,j):
            x[i][j] = sfix(i+j)
    x.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)

    # input2 = F.dropout(input1, p=p, training=True)

    # tensor.train()
    # tensor.reset_op_id()

    input2 = F.dropout(input1, p=p, training=True)
    input2.backward()
    input2.value.print_reveal_nested()
    input1.grad.print_reveal_nested()

@autograd_function
def test_f_batchnorm():
    print_ln('test_batchnorm')
    x = MultiArray([2,2,2,2], sfix)
    for i in range(0,2):
        for j in range(0,2):
            for k in range(0, 2):
                for q in range(0, 2):
                    x[i][j][k][q] = sfix(((i*j*k*q+i+j+k+q)%101)*1e-2)
    # x.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)

    w, b = Array(2, sfix), Array(2, sfix)
    w.assign_all(1)
    W = Tensor(w, req_grad = True)
    B = Tensor(b, req_grad = True)
    mean, std = Array(2, sfix), Array(2, sfix)
    M = Tensor(mean, req_grad = False)
    S = Tensor(std, req_grad = False)
    # input2 = F.batch_norm(input1, running_mean = M, running_var = S, 
    #                       weight=W, bias=B, training=True)

    # v = MultiArray([2,2,2,2], sfix)
    # for i in range(0,2):
    #     for j in range(0,2):
    #         for k in range(0, 2):
    #             for q in range(0, 2):
    #                 v[i][j][k][q] = sfix((i+j+k+q))
    # vec = Tensor(v, req_grad = True)
    # input4 = input2 * vec 

    # tensor.train()
    # tensor.reset_op_id()

    input2 = F.batch_norm(input1, running_mean = M, running_var = S,running_std = S, 
                          weight=None, bias=None, training=False)
    # input4 = input2 * vec

    # input4.backward()
    
    
    # print_ln('xhat_batchnorm_output: ')
    # input2.value.print_reveal_nested()
    # print_ln('xhat_batchnorm_backward: ')
    # input2.grad.print_reveal_nested()
    
    
    # print_ln('x_batchnorm_backward: ')
    # input1.grad.print_reveal_nested()

@autograd_function
def test_f_layernorm():
    print_ln('test_layernorm')
    x = MultiArray([2,2,2,2], sfix)
    for i in range(0,2):
        for j in range(0,2):
            for k in range(0, 2):
                for q in range(0, 2):
                    x[i][j][k][q] = sfix(((i*j*k*q+i+j+k+q)%101)*1e-2)
    x.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)

    w = MultiArray([2,2], sfix)
    b = MultiArray([2,2], sfix)
    w.assign_all(1)
    W = Tensor(w, req_grad = True)
    B = Tensor(b, req_grad = True)
    
    # input2 = F.layer_norm(input1, normalized_shape=[2,2], weight=W, bias=B)

    v = MultiArray([2,2,2,2], sfix)
    for i in range(0,2):
        for j in range(0,2):
            for k in range(0, 2):
                for q in range(0, 2):
                    v[i][j][k][q] = sfix((i+j+k+q))
    vec = Tensor(v, req_grad = True)
    # input4 = input2 * vec 

    # tensor.train()
    # tensor.reset_op_id()

    input2 = F.layer_norm(input1, normalized_shape=[2,2], weight=W, bias=B)
    input4 = input2 * vec

    input4.backward()
    
    
    print_ln('xhat_layernorm_output: ')
    input2.value.print_reveal_nested()
    print_ln('xhat_layernorm_backward: ')
    input2.grad.print_reveal_nested()
    
    
    print_ln('x_layernorm_backward: ')
    input1.grad.print_reveal_nested()

@autograd_function
def test_f_mseloss():
    print_ln('test_f_mseloss')
    
    dims = [3, 3]

    y = MultiArray(dims, sfix)
    for i in range(dims[0]):
        for j in range(dims[1]):
            y[i][j] = sfix(i+j)
    y.print_reveal_nested()
    Y = Tensor(y, req_grad = True)

    x = MultiArray(dims, sfix)
    for i in range(dims[0]):
        for j in range(dims[1]):
            x[i][j] = sfix(i*j)
    x.print_reveal_nested()
    label = Tensor(x, req_grad = False)
    
    # loss = F.mse_loss(Y, label)

    # tensor.train()
    # tensor.reset_op_id()

    loss = F.mse_loss(Y, label)
    loss.backward()
    loss.value.print_reveal_nested()
    Y.grad.print_reveal_nested()

@autograd_function
def test_f_normalize(p, dim=1):
    print_ln('test_f_normlize')
    
    dims = [3, 3]

    x = MultiArray(dims, sfix)
    for i in range(dims[0]):
        for j in range(dims[1]):
            x[i][j] = sfix(i+j)
    x.print_reveal_nested()
    X = Tensor(x, req_grad = True)
    
    # loss = F.normalize(X, p, dim)

    # tensor.train()
    # tensor.reset_op_id()

    loss = F.normalize(X, p, dim)
    loss.backward()
    loss.value.print_reveal_nested()
    X.grad.print_reveal_nested()

@autograd_function
def test_f_cossim():
    print_ln('test_f_cossim')
    
    dims = [3, 3]

    y = MultiArray(dims, sfix)
    for i in range(dims[0]):
        for j in range(dims[1]):
            y[i][j] = sfix(i+j)
    y.print_reveal_nested()
    Y = Tensor(y, req_grad = True)

    x = MultiArray(dims, sfix)
    for i in range(dims[0]):
        for j in range(dims[1]):
            x[i][j] = sfix(i*j)
    x.print_reveal_nested()
    label = Tensor(x, req_grad = False)
    
    # loss = F.cosine_similarity(Y, label)

    # tensor.train()
    # tensor.reset_op_id()

    loss = F.cosine_similarity(Y, label)
    loss.backward()
    loss.value.print_reveal_nested()
    Y.grad.print_reveal_nested()

@autograd_function
def test_hardtanh():
    print_ln('hardtanh')
    i, j = 3, 3
    x = MultiArray([3, 3], sfix)
    for i in range(0,3):
        for  j in range(0,3):
            x[i][j] = sfix(i-j)
    x.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)

    # input2 = F.gelu(input1, approximate='tanh')

    # tensor.train()
    # tensor.reset_op_id()

    input2 = input1.Hardtanh()
    input2.backward()
    input2.value.print_reveal_nested()
    input1.grad.print_reveal_nested()

@autograd_function
def test_f_gelu():
    print_ln('test_gelu')
    i, j = 3, 3
    x = MultiArray([3, 3], sfix)
    for i in range(0,3):
        for  j in range(0,3):
            x[i][j] = sfix(i-j)
    x.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)

    # input2 = F.gelu(input1, approximate='tanh')

    # tensor.train()
    # tensor.reset_op_id()

    input2 = F.gelu(input1, approximate='Hardtanh')
    #input2.backward()
    input2.value.print_reveal_nested()
    #input1.grad.print_reveal_nested()

@autograd_function
def test_flatten():
    print_ln('test_flatten')
    x = MultiArray([5, 4,3,2], sfix)
    for i in range(x.total_size()):
        x.assign_vector(sfix(i), i)
    x.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)

    # input2 = input1.flatten(1,2)

    # tensor.train()
    # tensor.reset_op_id()

    input2 = input1.flatten(1,2)
    input2.backward()
    input2.value.print_reveal_nested()
    input1.grad.print_reveal_nested()

@autograd_function
def test_gather():
    print_ln('test_gather')
    x = MultiArray([3,3], sfix)
    for i in range(x.total_size()):
        x.assign_vector(sfix(i+3), i)
    x.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)
    index = MultiArray([1,3], cint)
    for i in range(index.total_size()):
        if i == 2:
            index.assign_vector(cint(0), i)
        elif i == 0:
            index.assign_vector(cint(2), i)
        else:
            index.assign_vector(cint(i), i)
    index = Tensor(index)
    # input2 = input1.gather(0,index)

    # tensor.train()
    # tensor.reset_op_id()

    input2 = input1.gather(0,index)
    input2.backward()
    input2.value.print_reveal_nested()
    input1.grad.print_reveal_nested()

@autograd_function
def test_expand():
    print_ln('test_expand')
    x = MultiArray([1, 256], sfix)
    for i in range(0,1):
        for  j in range(0,256):
            x[i][j] = sfix(i-j)
    x.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)

    # input2 = input1.expand([50,256])

    # tensor.train()
    # tensor.reset_op_id()

    input2 = input1.expand([50,256])
    input2.backward()
    input2.value.print_reveal_nested()
    input1.grad.print_reveal_nested()

@autograd_function
def test_chunk():
    print_ln('test_chunk')
    x = MultiArray([3, 3, 3], sfix)
    for i in range(0,3):
        for  j in range(0,3):
            for  k in range(0,3):
                x[i][j][k] = sfix(i-j+k)
    x.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)

    # tensor.train()
    # tensor.reset_op_id()

    input2 = input1.chunk(2,dim=0)
    for i in range(0,2):
        input2[i].value.print_reveal_nested()
        
    for i in range(0,2):
        input2[i].backward()
    input1.grad.print_reveal_nested()

@autograd_function
def test_masked_fill_():
    print_ln('test_masked_fill_')
    x = MultiArray([3, 3], sfix)
    for i in range(0,3):
        for  j in range(0,3):
            x[i][j] = sfix(i-j)
    x.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)

    mask = MultiArray([3, 3], cint)
    for i in range(0,3):
        for  j in range(0,3):
            mask[i][j] = cint((i-j)%2)
    mask.print_reveal_nested()
    M = Tensor(mask, req_grad = False)

    value = MultiArray([3, 3], sfix)
    for i in range(0,3):
        for  j in range(0,3):
            value[i][j] = i+j
    value.print_reveal_nested()
    V = Tensor(value, req_grad = False)

    # input2 = input1.masked_fill_(M, V)

    # tensor.train()
    # tensor.reset_op_id()

    input2 = input1.masked_fill_(M, V)
    #input2.backward()
    input2.value.print_reveal_nested()
    #input1.grad.print_reveal_nested()
    
@autograd_function
def test_gt():
    print_ln('testgt')
    x = MultiArray([3, 3], sfix)
    for i in range(0,3):
        for  j in range(0,3):
            x[i][j] = sfix(i+j)
    x.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)

    y = MultiArray([1, 3], sfix)
    for i in range(0,1):
        for  j in range(0,3):
            y[i][j] = sfix(i*j)
    y.print_reveal_nested()
    input2 = Tensor(y, req_grad = True)

    # input3 = input1.gt(input2)

    # tensor.train()
    # tensor.reset_op_id()

    input3 = input1.gt(input2)
    #input2.backward()
    input3.value.print_reveal_nested()
    #input1.grad.print_reveal_nested()
    
@autograd_function
def test_triu():
    print_ln('test_triu')
    x = MultiArray([ 3,5], sfix)
    for i in range(x.total_size()):
         x.assign_vector(sfix(i), i)
    x.print_reveal_nested()
    input1 = Tensor(x, req_grad = True)

    y = Tensor.triu(input1, k=2)
    y.print_reveal_nested()
# test_mul()
#test_add()
#test_sub()
#test_div()
#test_abs()
#test_exp()
#test_log()
#test_pow(3)

@autograd_function
def test_getitem():
    # i, j = 3, 3
    x = Tensor.eye(10)
    tmp_value = MultiArray([10,1], sfix)
    for i in range(tmp_value.total_size()):
        tmp_value.assign_vector(sfix(i+1), i)
    y =Tensor(tmp_value)
    x[:, 0::2] = y
    x.print_reveal_nested()
    # x[0].print_reveal_nested()

@autograd_function
def test_arange():
    x = Tensor.arange(0, 20, 2)

    x.print_reveal_nested()
    # x[0].print_reveal_nested()    
# test_arange()
# test_getitem()
# test_argmax(dim=1)
# test_abs()
# test_exp()
# test_log()
# test_pow(3)
# test_invsqrt()

# # test_sum([0,2])
# test_sum([0,2], keepdim=True)
# test_mean([2])
# test_mean([0,2,3], keepdim=True)
test_var([3])
# test_var([0,2], keepdim=True)
# # test_std([0,2,3])
# test_std([0,2,3], keepdim=True)


# test_addc()
# test_subc()
# test_mulc()
# test_divc()
# test_combine()
# test_neg()
# program.use_trunc_pr = True

# test_hardtanh()

# test_f_droupout(0.5)
# test_f_batchnorm()
# test_f_layernorm()
#test_f_mseloss()
#test_f_normalize(p=2, dim=[0,1])
#test_f_cossim()
# test_f_gelu()

# test_masked_fill_()
# test_gather()
# test_expand()
# test_triu()
# test_getitem()
# test_chunk()

# x = MultiArray([2, 2], sfix)
# input = Tensor(x, req_grad = True)
# input.get_input_from(2)
# input.print_reveal_nested()